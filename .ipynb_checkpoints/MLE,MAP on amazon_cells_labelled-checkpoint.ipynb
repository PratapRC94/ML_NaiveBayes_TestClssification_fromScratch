{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MLE </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3\n",
      "60\n",
      "40\n",
      "[0.6]\n",
      "Task 3\n",
      "56\n",
      "44\n",
      "[0.6, 0.56]\n",
      "Task 3\n",
      "58\n",
      "42\n",
      "[0.6, 0.56, 0.58]\n",
      "Task 3\n",
      "65\n",
      "35\n",
      "[0.6, 0.56, 0.58, 0.65]\n",
      "Task 3\n",
      "62\n",
      "38\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62]\n",
      "Task 3\n",
      "60\n",
      "40\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62, 0.6]\n",
      "Task 3\n",
      "57\n",
      "43\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62, 0.6, 0.57]\n",
      "Task 3\n",
      "59\n",
      "41\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62, 0.6, 0.57, 0.59]\n",
      "Task 3\n",
      "63\n",
      "37\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62, 0.6, 0.57, 0.59, 0.63]\n",
      "Task 3\n",
      "73\n",
      "27\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62, 0.6, 0.57, 0.59, 0.63, 0.73]\n",
      "[0.6, 0.56, 0.58, 0.65, 0.62, 0.6, 0.57, 0.59, 0.63, 0.73]\n",
      "0.6130000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "f = open(\"amazon_cells_labelled.txt\", \"r\", encoding=\"utf-8\")\n",
    "data= f.read()\n",
    "\n",
    "data1=data.lower()\n",
    "data1= data1.split(\"\\n\")\n",
    "\n",
    "data_dic = dict()\n",
    "for i in range(len(data1)):\n",
    "    data_dic[i] = data1[i].split('\\t')\n",
    "    \n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "############################################TASK 7.1#########################################################\n",
    "pos_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '1':\n",
    "        pos_examples[i] = data_dic[i]\n",
    "\n",
    "neg_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '0':\n",
    "        neg_examples[i] = data_dic[i]\n",
    "###########################################TASK 7.2##########################################################\n",
    "accuracy= []\n",
    "for j in range(10):\n",
    "#i=0\n",
    "    pos_test = dict(list(pos_examples.items())[j*int((len(pos_examples))/10):(j*int((len(pos_examples))/10))+int((len(pos_examples))/10)])\n",
    "\n",
    "    pos_train = {}\n",
    "    for i in set(pos_examples.keys()) - set(pos_test.keys()):\n",
    "            pos_train[i] = pos_examples[i]\n",
    "\n",
    "#i=0\n",
    "    neg_test = dict(list(neg_examples.items())[j*int((len(neg_examples))/10):(j*int((len(neg_examples))/10))+int((len(neg_examples))/10)])\n",
    " \n",
    "\n",
    "    neg_train = {}\n",
    "    for i in set(neg_examples.keys()) - set(neg_test.keys()):\n",
    "            neg_train[i] = neg_examples[i]\n",
    "\n",
    "###########################################TASK 7.3############################################################\n",
    "    training_data = Merge(pos_train,neg_train)\n",
    "    test_data = Merge(pos_test,neg_test)\n",
    "\n",
    "#here we are finding number of total words in the whole dataset\n",
    "##############################################TASK 1##########################################################\n",
    "    total_words=[]\n",
    "    for i in range(len(training_data)):\n",
    "    \n",
    "        total_words=total_words+data_dic[i][0].split()\n",
    "\n",
    "\n",
    "    words_set = set(total_words)\n",
    "    vocab = len(words_set)\n",
    "    vocab_lt=list(words_set)\n",
    "\n",
    "#############################################TASK 2###########################################################\n",
    "    words_1=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '1' :\n",
    "            words_1 = words_1 + data_dic[i][0].split()\n",
    "\n",
    "    words_0=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '0' :\n",
    "            words_0 = words_0 + data_dic[i][0].split()\n",
    "\n",
    "#############################################TASK 3###########################################################\n",
    "    print('Task 3')\n",
    "    count0 = 0\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '0':\n",
    "            count0 = count0 + 1\n",
    "\n",
    "    p_neg = count0/len(training_data)\n",
    "\n",
    "    count1 = 0\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '1':\n",
    "            count1 = count1 + 1\n",
    "\n",
    "    p_pos = count1/len(training_data)\n",
    "\n",
    "############################################TASK 4##############################################################\n",
    "    neg_words=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '0':\n",
    "            neg_words = neg_words + training_data[i][0].split()\n",
    "\n",
    "    neg_words_set= set(neg_words)\n",
    "    vocab_not_in_neg_words_set = words_set - neg_words_set\n",
    "\n",
    "    vocab_not_in_neg_words_count_dic = {}\n",
    "    for i in vocab_not_in_neg_words_set:\n",
    "        vocab_not_in_neg_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "    neg_counts1= dict()\n",
    "    for word in neg_words:\n",
    "            if word in neg_counts1:\n",
    "                neg_counts1[word] += 1\n",
    "            else:\n",
    "                neg_counts1[word] = 1\n",
    "\n",
    "    neg_counts = Merge(vocab_not_in_neg_words_count_dic,neg_counts1)\n",
    "\n",
    "\n",
    "    neg_prob = {}\n",
    "    for key in neg_counts:\n",
    "        neg_prob[key] = neg_counts[key]/len(neg_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pos_words=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '1':\n",
    "            pos_words = pos_words + training_data[i][0].split()\n",
    "     \n",
    "    pos_words_set= set(pos_words)\n",
    "    vocab_not_in_pos_words_set = words_set - pos_words_set\n",
    "\n",
    "    vocab_not_in_pos_words_count_dic = {}\n",
    "    for i in vocab_not_in_pos_words_set:\n",
    "        vocab_not_in_pos_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "    pos_counts1= dict()\n",
    "    for word in pos_words:\n",
    "            if word in pos_counts1:\n",
    "                pos_counts1[word] += 1\n",
    "            else:\n",
    "                pos_counts1[word] = 1\n",
    "\n",
    "    pos_counts = Merge(vocab_not_in_pos_words_count_dic,pos_counts1)\n",
    "\n",
    "    pos_prob = {}\n",
    "    for key in pos_counts:\n",
    "        pos_prob[key] = pos_counts[key]/len(pos_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    estimate_result = []\n",
    "    for i in set(test_data.keys()):\n",
    "        a= test_data[i][0]\n",
    "        c = a.lower()\n",
    "        b = c.split()\n",
    "        skippable_words_set = set(b) - words_set\n",
    "        b = set(b)- skippable_words_set\n",
    "        \n",
    "\n",
    "        neg_prob_score1= 1\n",
    "        for neg_words in b:\n",
    "            neg_prob_score1 = neg_prob_score1 * neg_prob[neg_words]\n",
    "        neg_prob_score = neg_prob_score1 * p_neg\n",
    "\n",
    "        pos_prob_score1= 1\n",
    "        for pos_words in b:\n",
    "            pos_prob_score1 = pos_prob_score1 * pos_prob[pos_words]\n",
    "        pos_prob_score = pos_prob_score1 * p_pos\n",
    "\n",
    " \n",
    "        if pos_prob_score > neg_prob_score:\n",
    "            estimate_result = estimate_result + ['1']\n",
    "        else:\n",
    "            estimate_result = estimate_result + ['0']\n",
    "\n",
    "    actual_result = []\n",
    "    for i in set(test_data.keys()):\n",
    "        actual_result = actual_result + list(test_data[i][1])\n",
    "\n",
    "    matched=0\n",
    "    unmatched =0\n",
    "    for i in range(len(actual_result)):\n",
    "        if actual_result[i]==estimate_result[i]:\n",
    "            matched=matched+1\n",
    "        else:\n",
    "            unmatched=unmatched+1\n",
    "    print(matched)\n",
    "    print(unmatched)\n",
    "    accuracy = accuracy+[((matched)/(matched+unmatched))]\n",
    "    print(accuracy)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "sum_of_accuracy = 0\n",
    "for i in range(len(accuracy)):\n",
    "    sum_of_accuracy = sum_of_accuracy + accuracy[i]\n",
    "    \n",
    "avg_accuracy = sum_of_accuracy/len(accuracy)\n",
    "print(avg_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MAP </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3\n",
      "82\n",
      "18\n",
      "[0.82]\n",
      "Task 3\n",
      "79\n",
      "21\n",
      "[0.82, 0.79]\n",
      "Task 3\n",
      "83\n",
      "17\n",
      "[0.82, 0.79, 0.83]\n",
      "Task 3\n",
      "79\n",
      "21\n",
      "[0.82, 0.79, 0.83, 0.79]\n",
      "Task 3\n",
      "77\n",
      "23\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77]\n",
      "Task 3\n",
      "73\n",
      "27\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77, 0.73]\n",
      "Task 3\n",
      "75\n",
      "25\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77, 0.73, 0.75]\n",
      "Task 3\n",
      "76\n",
      "24\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77, 0.73, 0.75, 0.76]\n",
      "Task 3\n",
      "83\n",
      "17\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77, 0.73, 0.75, 0.76, 0.83]\n",
      "Task 3\n",
      "75\n",
      "25\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77, 0.73, 0.75, 0.76, 0.83, 0.75]\n",
      "[0.82, 0.79, 0.83, 0.79, 0.77, 0.73, 0.75, 0.76, 0.83, 0.75]\n",
      "0.782\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "f = open(\"amazon_cells_labelled.txt\", \"r\", encoding=\"utf-8\")\n",
    "data= f.read()\n",
    "\n",
    "data1=data.lower()\n",
    "data1= data1.split(\"\\n\")\n",
    "\n",
    "data_dic = dict()\n",
    "for i in range(len(data1)):\n",
    "    data_dic[i] = data1[i].split('\\t')\n",
    "    \n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "############################################TASK 7.1#########################################################\n",
    "pos_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '1':\n",
    "        pos_examples[i] = data_dic[i]\n",
    "\n",
    "neg_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '0':\n",
    "        neg_examples[i] = data_dic[i]\n",
    "###########################################TASK 7.2##########################################################\n",
    "accuracy= []\n",
    "for j in range(10):\n",
    "#i=0\n",
    "    pos_test = dict(list(pos_examples.items())[j*int((len(pos_examples))/10):(j*int((len(pos_examples))/10))+int((len(pos_examples))/10)])\n",
    "\n",
    "    pos_train = {}\n",
    "    for i in set(pos_examples.keys()) - set(pos_test.keys()):\n",
    "            pos_train[i] = pos_examples[i]\n",
    "\n",
    "#i=0\n",
    "    neg_test = dict(list(neg_examples.items())[j*int((len(neg_examples))/10):(j*int((len(neg_examples))/10))+int((len(neg_examples))/10)])\n",
    " \n",
    "\n",
    "    neg_train = {}\n",
    "    for i in set(neg_examples.keys()) - set(neg_test.keys()):\n",
    "            neg_train[i] = neg_examples[i]\n",
    "\n",
    "###########################################TASK 7.3############################################################\n",
    "    training_data = Merge(pos_train,neg_train)\n",
    "    test_data = Merge(pos_test,neg_test)\n",
    "\n",
    "#here we are finding number of total words in the whole dataset\n",
    "##############################################TASK 1##########################################################\n",
    "    total_words=[]\n",
    "    for i in range(len(training_data)):\n",
    "    \n",
    "        total_words=total_words+data_dic[i][0].split()\n",
    "\n",
    "\n",
    "    words_set = set(total_words)\n",
    "    vocab = len(words_set)\n",
    "    vocab_lt=list(words_set)\n",
    "\n",
    "#############################################TASK 2###########################################################\n",
    "    words_1=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '1' :\n",
    "            words_1 = words_1 + data_dic[i][0].split()\n",
    "\n",
    "    words_0=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '0' :\n",
    "            words_0 = words_0 + data_dic[i][0].split()\n",
    "\n",
    "#############################################TASK 3###########################################################\n",
    "    print('Task 3')\n",
    "    count0 = 0\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '0':\n",
    "            count0 = count0 + 1\n",
    "\n",
    "    p_neg = count0/len(training_data)\n",
    "\n",
    "    count1 = 0\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '1':\n",
    "            count1 = count1 + 1\n",
    "\n",
    "    p_pos = count1/len(training_data)\n",
    "\n",
    "############################################TASK 4##############################################################\n",
    "    neg_words=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '0':\n",
    "            neg_words = neg_words + training_data[i][0].split()\n",
    "\n",
    "    neg_words_set= set(neg_words)\n",
    "    vocab_not_in_neg_words_set = words_set - neg_words_set\n",
    "\n",
    "    vocab_not_in_neg_words_count_dic = {}\n",
    "    for i in vocab_not_in_neg_words_set:\n",
    "        vocab_not_in_neg_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "    neg_counts1= dict()\n",
    "    for word in neg_words:\n",
    "            if word in neg_counts1:\n",
    "                neg_counts1[word] += 1\n",
    "            else:\n",
    "                neg_counts1[word] = 1\n",
    "\n",
    "    neg_counts = Merge(vocab_not_in_neg_words_count_dic,neg_counts1)\n",
    "\n",
    "\n",
    "    map_neg_prob ={}\n",
    "    for key in neg_counts:\n",
    "        map_neg_prob[key] = (neg_counts[key]+1)/(len(neg_words)+len(words_set))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pos_words=[]\n",
    "    for i in set(training_data.keys()):\n",
    "        if training_data[i][1] == '1':\n",
    "            pos_words = pos_words + training_data[i][0].split()\n",
    "     \n",
    "    pos_words_set= set(pos_words)\n",
    "    vocab_not_in_pos_words_set = words_set - pos_words_set\n",
    "\n",
    "    vocab_not_in_pos_words_count_dic = {}\n",
    "    for i in vocab_not_in_pos_words_set:\n",
    "        vocab_not_in_pos_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "    pos_counts1= dict()\n",
    "    for word in pos_words:\n",
    "            if word in pos_counts1:\n",
    "                pos_counts1[word] += 1\n",
    "            else:\n",
    "                pos_counts1[word] = 1\n",
    "\n",
    "    pos_counts = Merge(vocab_not_in_pos_words_count_dic,pos_counts1)\n",
    "\n",
    "    map_pos_prob ={}\n",
    "    for key in pos_counts:\n",
    "        map_pos_prob[key] = (pos_counts[key]+1)/(len(pos_words)+len(words_set))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    estimate_result = []\n",
    "    for i in set(test_data.keys()):\n",
    "        a= test_data[i][0]\n",
    "        c = a.lower()\n",
    "        b = c.split()\n",
    "        skippable_words_set = set(b) - words_set\n",
    "        b = set(b)- skippable_words_set\n",
    "        \n",
    "\n",
    "        neg_prob_score1= 1\n",
    "        for neg_words in b:\n",
    "            neg_prob_score1 = neg_prob_score1 * map_neg_prob[neg_words]\n",
    "        neg_prob_score = neg_prob_score1 * p_neg\n",
    "\n",
    "        pos_prob_score1= 1\n",
    "        for pos_words in b:\n",
    "            pos_prob_score1 = pos_prob_score1 * map_pos_prob[pos_words]\n",
    "        pos_prob_score = pos_prob_score1 * p_pos\n",
    "\n",
    " \n",
    "        if pos_prob_score > neg_prob_score:\n",
    "            estimate_result = estimate_result + ['1']\n",
    "        else:\n",
    "            estimate_result = estimate_result + ['0']\n",
    "\n",
    "    actual_result = []\n",
    "    for i in set(test_data.keys()):\n",
    "        actual_result = actual_result + list(test_data[i][1])\n",
    "\n",
    "    matched=0\n",
    "    unmatched =0\n",
    "    for i in range(len(actual_result)):\n",
    "        if actual_result[i]==estimate_result[i]:\n",
    "            matched=matched+1\n",
    "        else:\n",
    "            unmatched=unmatched+1\n",
    "    print(matched)\n",
    "    print(unmatched)\n",
    "    accuracy = accuracy+[((matched)/(matched+unmatched))]\n",
    "    print(accuracy)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "sum_of_accuracy = 0\n",
    "for i in range(len(accuracy)):\n",
    "    sum_of_accuracy = sum_of_accuracy + accuracy[i]\n",
    "    \n",
    "avg_accuracy = sum_of_accuracy/len(accuracy)\n",
    "print(avg_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> K_fold MLE accuracies for k = 2,3,4,5,6,7,8,9,10 Final</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3666666666666667, 0.4212962962962963, 0.4671428571428572, 0.5026666666666667, 0.5300751879699247, 0.5537190082644629, 0.5803571428571428, 0.5989417989417989, 0.6130000000000001]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "f = open(\"amazon_cells_labelled.txt\", \"r\", encoding=\"utf-8\")\n",
    "data= f.read()\n",
    "\n",
    "data1=data.lower()\n",
    "data1= data1.split(\"\\n\")\n",
    "\n",
    "data_dic = dict()\n",
    "for i in range(len(data1)):\n",
    "    data_dic[i] = data1[i].split('\\t')\n",
    "    \n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "############################################TASK 7.1#########################################################\n",
    "pos_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '1':\n",
    "        pos_examples[i] = data_dic[i]\n",
    "\n",
    "neg_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '0':\n",
    "        neg_examples[i] = data_dic[i]\n",
    "k=[2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "avg_accuracy_for_k_fold = []\n",
    "for q in k:\n",
    "    accuracy= []\n",
    "    for j in range(q):\n",
    "    #i=0\n",
    "        pos_test = dict(list(pos_examples.items())[j*int((len(pos_examples))/q):(j*int((len(pos_examples))/q))+int((len(pos_examples))/q)])\n",
    "\n",
    "        pos_train = {}\n",
    "        for i in set(pos_examples.keys()) - set(pos_test.keys()):\n",
    "                pos_train[i] = pos_examples[i]\n",
    "\n",
    "    #i=0\n",
    "        neg_test = dict(list(neg_examples.items())[j*int((len(neg_examples))/10):(j*int((len(neg_examples))/10))+int((len(neg_examples))/10)])\n",
    " \n",
    "\n",
    "        neg_train = {}\n",
    "        for i in set(neg_examples.keys()) - set(neg_test.keys()):\n",
    "                neg_train[i] = neg_examples[i]\n",
    "\n",
    "###########################################TASK 7.3############################################################\n",
    "        training_data = Merge(pos_train,neg_train)\n",
    "        test_data = Merge(pos_test,neg_test)\n",
    "\n",
    "#here we are finding number of total words in the whole dataset\n",
    "##############################################TASK 1##########################################################\n",
    "        total_words=[]\n",
    "        for i in range(len(training_data)):\n",
    "    \n",
    "            total_words=total_words+data_dic[i][0].split()\n",
    "\n",
    "\n",
    "        words_set = set(total_words)\n",
    "        vocab = len(words_set)\n",
    "        vocab_lt=list(words_set)\n",
    "\n",
    "#############################################TASK 2###########################################################\n",
    "        words_1=[]\n",
    "        for i in set(training_data.keys()):\n",
    "            if training_data[i][1] == '1' :\n",
    "                words_1 = words_1 + data_dic[i][0].split()\n",
    "\n",
    "        words_0=[]\n",
    "        for i in set(training_data.keys()):\n",
    "            if training_data[i][1] == '0' :\n",
    "                words_0 = words_0 + data_dic[i][0].split()\n",
    "\n",
    "#############################################TASK 3###########################################################\n",
    "     #   print('Task 3')\n",
    "        count0 = 0\n",
    "        for i in set(training_data.keys()):\n",
    "            if training_data[i][1] == '0':\n",
    "                count0 = count0 + 1\n",
    "\n",
    "        p_neg = count0/len(training_data)\n",
    "\n",
    "        count1 = 0\n",
    "        for i in set(training_data.keys()):\n",
    "            if training_data[i][1] == '1':\n",
    "                count1 = count1 + 1\n",
    "\n",
    "        p_pos = count1/len(training_data)\n",
    "\n",
    "############################################TASK 4##############################################################\n",
    "        neg_words=[]\n",
    "        for i in set(training_data.keys()):\n",
    "            if training_data[i][1] == '0':\n",
    "                neg_words = neg_words + training_data[i][0].split()\n",
    "\n",
    "        neg_words_set= set(neg_words)\n",
    "        vocab_not_in_neg_words_set = words_set - neg_words_set\n",
    "\n",
    "        vocab_not_in_neg_words_count_dic = {}\n",
    "        for i in vocab_not_in_neg_words_set:\n",
    "            vocab_not_in_neg_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "        neg_counts1= dict()\n",
    "        for word in neg_words:\n",
    "                if word in neg_counts1:\n",
    "                    neg_counts1[word] += 1\n",
    "                else:\n",
    "                    neg_counts1[word] = 1\n",
    "\n",
    "        neg_counts = Merge(vocab_not_in_neg_words_count_dic,neg_counts1)\n",
    "\n",
    "\n",
    "        neg_prob = {}\n",
    "        for key in neg_counts:\n",
    "            neg_prob[key] = neg_counts[key]/len(neg_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        pos_words=[]\n",
    "        for i in set(training_data.keys()):\n",
    "            if training_data[i][1] == '1':\n",
    "                pos_words = pos_words + training_data[i][0].split()\n",
    "     \n",
    "        pos_words_set= set(pos_words)\n",
    "        vocab_not_in_pos_words_set = words_set - pos_words_set\n",
    "\n",
    "        vocab_not_in_pos_words_count_dic = {}\n",
    "        for i in vocab_not_in_pos_words_set:\n",
    "            vocab_not_in_pos_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "        pos_counts1= dict()\n",
    "        for word in pos_words:\n",
    "                if word in pos_counts1:\n",
    "                    pos_counts1[word] += 1\n",
    "                else:\n",
    "                    pos_counts1[word] = 1\n",
    "\n",
    "        pos_counts = Merge(vocab_not_in_pos_words_count_dic,pos_counts1)\n",
    "\n",
    "        pos_prob = {}\n",
    "        for key in pos_counts:\n",
    "            pos_prob[key] = pos_counts[key]/len(pos_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        estimate_result = []\n",
    "        for i in set(test_data.keys()):\n",
    "            a= test_data[i][0]\n",
    "            c = a.lower()\n",
    "            b = c.split()\n",
    "            skippable_words_set = set(b) - words_set\n",
    "            b = set(b)- skippable_words_set\n",
    "        \n",
    "\n",
    "            neg_prob_score1= 1\n",
    "            for neg_words in b:\n",
    "                neg_prob_score1 = neg_prob_score1 * neg_prob[neg_words]\n",
    "            neg_prob_score = neg_prob_score1 * p_neg\n",
    "\n",
    "            pos_prob_score1= 1\n",
    "            for pos_words in b:\n",
    "                pos_prob_score1 = pos_prob_score1 * pos_prob[pos_words]\n",
    "            pos_prob_score = pos_prob_score1 * p_pos\n",
    "\n",
    " \n",
    "            if pos_prob_score > neg_prob_score:\n",
    "                estimate_result = estimate_result + ['1']\n",
    "            else:\n",
    "                estimate_result = estimate_result + ['0']\n",
    "\n",
    "        actual_result = []\n",
    "        for i in set(test_data.keys()):\n",
    "            actual_result = actual_result + list(test_data[i][1])\n",
    "\n",
    "        matched=0\n",
    "        unmatched =0\n",
    "        for i in range(len(actual_result)):\n",
    "            if actual_result[i]==estimate_result[i]:\n",
    "                matched=matched+1\n",
    "            else:\n",
    "                unmatched=unmatched+1\n",
    "        accuracy = accuracy+[((matched)/(matched+unmatched))]\n",
    "\n",
    "\n",
    "    sum_of_accuracy = 0\n",
    "    for i in range(len(accuracy)):\n",
    "        sum_of_accuracy = sum_of_accuracy + accuracy[i]\n",
    "    \n",
    "    avg_accuracy = sum_of_accuracy/len(accuracy)\n",
    "    #print(avg_accuracy)\n",
    "    avg_accuracy_for_k_fold = avg_accuracy_for_k_fold + [avg_accuracy]\n",
    "\n",
    "print(avg_accuracy_for_k_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MAP K_fold accuracies for k = 2,3,4,5,6,7,8,9 Final</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6783333333333333, 0.7391975308641975, 0.7542857142857143, 0.764, 0.7581453634085213, 0.7685950413223139, 0.7689732142857143, 0.7629629629629631, 0.7780000000000001], [0.6866666666666666, 0.7391975308641975, 0.7671428571428571, 0.7719999999999999, 0.7694235588972432, 0.7827626918536009, 0.7834821428571429, 0.7735449735449735, 0.788], [0.6883333333333332, 0.7469135802469137, 0.7742857142857142, 0.776, 0.7719298245614036, 0.7815820543093271, 0.7845982142857144, 0.7756613756613757, 0.784], [0.6883333333333334, 0.7530864197530865, 0.7785714285714287, 0.78, 0.7756892230576441, 0.7863046044864228, 0.7901785714285715, 0.7777777777777778, 0.7789999999999999], [0.6866666666666668, 0.7561728395061729, 0.7757142857142858, 0.7826666666666666, 0.7769423558897243, 0.7863046044864228, 0.7901785714285714, 0.782010582010582, 0.7849999999999999], [0.6933333333333334, 0.7561728395061729, 0.7771428571428571, 0.7853333333333332, 0.7756892230576441, 0.7863046044864228, 0.7912946428571429, 0.7851851851851852, 0.7859999999999999], [0.6916666666666667, 0.757716049382716, 0.7828571428571428, 0.784, 0.7719298245614036, 0.7851239669421487, 0.7901785714285714, 0.7841269841269841, 0.781], [0.6950000000000001, 0.7638888888888888, 0.7771428571428571, 0.788, 0.7744360902255639, 0.7839433293978748, 0.7868303571428572, 0.782010582010582, 0.7789999999999999], [0.7, 0.7654320987654321, 0.7771428571428571, 0.7866666666666667, 0.7781954887218044, 0.780401416765053, 0.7834821428571428, 0.780952380952381, 0.782]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "f = open(\"amazon_cells_labelled.txt\", \"r\", encoding=\"utf-8\")\n",
    "data= f.read()\n",
    "\n",
    "data1=data.lower()\n",
    "data1= data1.split(\"\\n\")\n",
    "\n",
    "data_dic = dict()\n",
    "for i in range(len(data1)):\n",
    "    data_dic[i] = data1[i].split('\\t')\n",
    "    \n",
    "\n",
    "def Merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res\n",
    "############################################TASK 7.1#########################################################\n",
    "pos_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '1':\n",
    "        pos_examples[i] = data_dic[i]\n",
    "\n",
    "neg_examples = {}\n",
    "for i in range(len(data_dic)-1):\n",
    "    if data_dic[i][1] == '0':\n",
    "        neg_examples[i] = data_dic[i]\n",
    "m = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "k=[2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "avg_accuracy_parameter_wise = []\n",
    "for p in m :\n",
    "    avg_accuracy_for_k_fold = []\n",
    "    for q in k:\n",
    "        accuracy= []\n",
    "        for j in range(q):\n",
    "        #i=0\n",
    "            pos_test = dict(list(pos_examples.items())[j*int((len(pos_examples))/q):(j*int((len(pos_examples))/q))+int((len(pos_examples))/q)])\n",
    "\n",
    "            pos_train = {}\n",
    "            for i in set(pos_examples.keys()) - set(pos_test.keys()):\n",
    "                    pos_train[i] = pos_examples[i]\n",
    "\n",
    "        #i=0\n",
    "            neg_test = dict(list(neg_examples.items())[j*int((len(neg_examples))/10):(j*int((len(neg_examples))/10))+int((len(neg_examples))/10)])\n",
    " \n",
    "\n",
    "            neg_train = {}\n",
    "            for i in set(neg_examples.keys()) - set(neg_test.keys()):\n",
    "                    neg_train[i] = neg_examples[i]\n",
    "\n",
    "###########################################TASK 7.3############################################################\n",
    "            training_data = Merge(pos_train,neg_train)\n",
    "            test_data = Merge(pos_test,neg_test)\n",
    "\n",
    "#here we are finding number of total words in the whole dataset\n",
    "##############################################TASK 1##########################################################\n",
    "            total_words=[]\n",
    "            for i in range(len(training_data)):\n",
    "    \n",
    "                total_words=total_words+data_dic[i][0].split()\n",
    "\n",
    "\n",
    "            words_set = set(total_words)\n",
    "            vocab = len(words_set)\n",
    "            vocab_lt=list(words_set)\n",
    "\n",
    "#############################################TASK 2###########################################################\n",
    "            words_1=[]\n",
    "            for i in set(training_data.keys()):\n",
    "                if training_data[i][1] == '1' :\n",
    "                    words_1 = words_1 + data_dic[i][0].split()\n",
    "\n",
    "            words_0=[]\n",
    "            for i in set(training_data.keys()):\n",
    "                if training_data[i][1] == '0' :\n",
    "                    words_0 = words_0 + data_dic[i][0].split()\n",
    "\n",
    "#############################################TASK 3###########################################################\n",
    "       #     print('Task 3')\n",
    "            count0 = 0\n",
    "            for i in set(training_data.keys()):\n",
    "                if training_data[i][1] == '0':\n",
    "                    count0 = count0 + 1\n",
    "\n",
    "            p_neg = count0/len(training_data)\n",
    "\n",
    "            count1 = 0\n",
    "            for i in set(training_data.keys()):\n",
    "                if training_data[i][1] == '1':\n",
    "                    count1 = count1 + 1\n",
    "\n",
    "            p_pos = count1/len(training_data)\n",
    "\n",
    "############################################TASK 4##############################################################\n",
    "            neg_words=[]\n",
    "            for i in set(training_data.keys()):\n",
    "                if training_data[i][1] == '0':\n",
    "                    neg_words = neg_words + training_data[i][0].split()\n",
    "\n",
    "            neg_words_set= set(neg_words)\n",
    "            vocab_not_in_neg_words_set = words_set - neg_words_set\n",
    "\n",
    "            vocab_not_in_neg_words_count_dic = {}\n",
    "            for i in vocab_not_in_neg_words_set:\n",
    "                vocab_not_in_neg_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "            neg_counts1= dict()\n",
    "            for word in neg_words:\n",
    "                    if word in neg_counts1:\n",
    "                        neg_counts1[word] += 1\n",
    "                    else:\n",
    "                        neg_counts1[word] = 1\n",
    "\n",
    "            neg_counts = Merge(vocab_not_in_neg_words_count_dic,neg_counts1)\n",
    "\n",
    "\n",
    "            map_neg_prob ={}\n",
    "            for key in neg_counts:\n",
    "                map_neg_prob[key] = (neg_counts[key]+p)/(len(neg_words)+len(words_set))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            pos_words=[]\n",
    "            for i in set(training_data.keys()):\n",
    "                if training_data[i][1] == '1':\n",
    "                    pos_words = pos_words + training_data[i][0].split()\n",
    "     \n",
    "            pos_words_set= set(pos_words)\n",
    "            vocab_not_in_pos_words_set = words_set - pos_words_set\n",
    "\n",
    "            vocab_not_in_pos_words_count_dic = {}\n",
    "            for i in vocab_not_in_pos_words_set:\n",
    "                vocab_not_in_pos_words_count_dic[i] = 0\n",
    "\n",
    "\n",
    "            pos_counts1= dict()\n",
    "            for word in pos_words:\n",
    "                    if word in pos_counts1:\n",
    "                        pos_counts1[word] += 1\n",
    "                    else:\n",
    "                        pos_counts1[word] = 1\n",
    "\n",
    "            pos_counts = Merge(vocab_not_in_pos_words_count_dic,pos_counts1)\n",
    "\n",
    "            map_pos_prob ={}\n",
    "            for key in pos_counts:\n",
    "                map_pos_prob[key] = (pos_counts[key]+p)/(len(pos_words)+len(words_set))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            estimate_result = []\n",
    "            for i in set(test_data.keys()):\n",
    "                a= test_data[i][0]\n",
    "                c = a.lower()\n",
    "                b = c.split()\n",
    "                skippable_words_set = set(b) - words_set\n",
    "                b = set(b)- skippable_words_set\n",
    "        \n",
    "\n",
    "                neg_prob_score1= 1\n",
    "                for neg_words in b:\n",
    "                    neg_prob_score1 = neg_prob_score1 * map_neg_prob[neg_words]\n",
    "                neg_prob_score = neg_prob_score1 * p_neg\n",
    "\n",
    "                pos_prob_score1= 1\n",
    "                for pos_words in b:\n",
    "                    pos_prob_score1 = pos_prob_score1 * map_pos_prob[pos_words]\n",
    "                pos_prob_score = pos_prob_score1 * p_pos\n",
    "\n",
    " \n",
    "                if pos_prob_score > neg_prob_score:\n",
    "                    estimate_result = estimate_result + ['1']\n",
    "                else:\n",
    "                    estimate_result = estimate_result + ['0']\n",
    "\n",
    "            actual_result = []\n",
    "            for i in set(test_data.keys()):\n",
    "                actual_result = actual_result + list(test_data[i][1])\n",
    "\n",
    "            matched=0\n",
    "            unmatched =0\n",
    "            for i in range(len(actual_result)):\n",
    "                if actual_result[i]==estimate_result[i]:\n",
    "                    matched=matched+1\n",
    "                else:\n",
    "                    unmatched=unmatched+1\n",
    "            accuracy = accuracy+[((matched)/(matched+unmatched))]\n",
    "\n",
    "\n",
    "        sum_of_accuracy = 0\n",
    "        for i in range(len(accuracy)):\n",
    "            sum_of_accuracy = sum_of_accuracy + accuracy[i]\n",
    "    \n",
    "        avg_accuracy = sum_of_accuracy/len(accuracy)\n",
    "       # print(avg_accuracy)\n",
    "        avg_accuracy_for_k_fold = avg_accuracy_for_k_fold + [avg_accuracy]\n",
    "\n",
    "   # print(avg_accuracy_for_k_fold)\n",
    "    avg_accuracy_parameter_wise = avg_accuracy_parameter_wise + [avg_accuracy_for_k_fold]\n",
    "\n",
    "print(avg_accuracy_parameter_wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
